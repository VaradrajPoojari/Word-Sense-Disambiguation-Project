{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Worked on automatic word sense disambiguation (WSD) using the context around an ambigious word\n",
    "- Applied the semantic knowledge in WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "Downloaded the following required NLTK corpora/lexicons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package senseval to\n",
      "[nltk_data]     /Users/varadrajrameshpoojary/nltk_data...\n",
      "[nltk_data]   Package senseval is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/varadrajrameshpoojary/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/varadrajrameshpoojary/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/varadrajrameshpoojary/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/varadrajrameshpoojary/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"senseval\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import senseval, stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.wsd import lesk\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "OPEN_CLASS_POS = {\"n\", \"v\", \"j\", \"r\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the WSD task\n",
    "\n",
    "We use the Senseval corpus included in NLTK, which has sense-tagged data for a small set of word types. We only look at the ambiguity of the word *line*. Note that this corpus is arranged in a way that is **NOT** typical for NLTK corpora. It is stored in a list of *instances*, where each instance has the sense and the context around it. We iterate over instances of the word *line* using this code: `for instance in senseval.instances('line.pos')`.\n",
    "\n",
    "We reorganize the information into two Python dictionaries: *train* and *test*.\n",
    "Each dictionary will contain senses as the keys, while the values are lists of POS-tagged sentences (if an *instance* in the semeval corpus has the given sense, it is included in this list).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = defaultdict(list)\n",
    "test_dict = defaultdict(list)\n",
    "# Your code here\n",
    "for instance in senseval.instances(\"line.pos\"):\n",
    "    sense = instance.senses[0]\n",
    "    context = instance.context\n",
    "    if len(test_dict[sense]) < 200:\n",
    "        test_dict[sense].append(context)\n",
    "    else:\n",
    "        train_dict[sense].append(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "assert len(test_dict.keys()) == 6\n",
    "assert len(train_dict.keys()) == 6\n",
    "assert \"product\" in test_dict.keys()\n",
    "assert \"product\" in train_dict.keys()\n",
    "assert len(test_dict[\"product\"]) == 200\n",
    "assert len(train_dict[\"product\"]) == 2017\n",
    "assert len(train_dict[\"product\"][0]) == 49\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and testing features for WSD\n",
    "\n",
    "We will be extracting features from the semeval data stored in Part 1.  We will be extracting several different types of features to eventually present to a classifier to do word-sense disambiguation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concreteness feature\n",
    "\n",
    "One typical distinction between senses of a word is that some senses are more concrete (involving the physical world) whereas others are more abstract.  For example, \"house\" is very concrete - it is a thing that exists in the world, while \"happiness\" is abstract - there are many different definitions, and you can't point to something and say \"that's happiness\". A list of words with human-assigned concreteness ratings can be found on the webpage [here](https://raw.githubusercontent.com/ArtsEngine/concreteness/master/Concreteness_ratings_Brysbaert_et_al_BRM.txt); the relevant column is *Conc.M*. Note that they are floating-point numbers (0 means no value was assigned).We extract this information into a Python dict (key is word, value is concreteness) and then write a function which calculates an average concreteness score for all words in a context (that is, given a list of context words, your function calculates the average concreteness of all of them).We lemmatize and lowercase the words in the context before you look them up in the dictionary.  If a word occurs more than once, it should be counted more than once. If a word has no concreteness score (ie, Conc.M == 0) it should be left out of the calculation (both numerator and denominator).\n",
    "\n",
    "For example, in the sentence \"This is a test\", we get:\n",
    "\n",
    "this = 2.14 <br/>\n",
    "is = 1.59 <br/>\n",
    "a = 1.46 <br/>\n",
    "test = 3.93 <br/>\n",
    "\n",
    "So the concreteness score should be (2.14 + 1.59 + 1.46 + 3.93) / 4 = 2.28\n",
    "\n",
    "\n",
    "Then use this function to show that the \"cord\" sense of *line* appears in more concrete contexts, on average, than the \"division\" sense. We use the function you've built, averaging the result across all the contexts for each of those two senses (using the training data from part 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/ArtsEngine/concreteness/master/Concreteness_ratings_Brysbaert_et_al_BRM.txt\",\n",
    "    delimiter=\"\\t\",\n",
    "    index_col=0,\n",
    ")\n",
    "conc_dict = df[\"Conc.M\"].to_dict()\n",
    "\n",
    "\n",
    "def get_conc_score(context):\n",
    "    \"\"\"calculate the average concreteness score for all words in a given context\"\"\"\n",
    "    # Your code here\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for element in context:\n",
    "        # Skip ill-formatted word-pos pairs\n",
    "        if type(element) != tuple or len(element) != 2:\n",
    "            continue\n",
    "        word, _ = element\n",
    "        lemma = lemmatizer.lemmatize(word.lower())\n",
    "        if lemma in conc_dict and conc_dict[lemma] != 0:\n",
    "            total += conc_dict[lemma]\n",
    "            count += 1\n",
    "    score = total / count\n",
    "    return score\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = pos_tag(word_tokenize(\"I have a cat\"))\n",
    "assert get_conc_score(test_context) == ((2.18 + 1.46 + 4.86) / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The concreteness score for 'cord' is: 2.826\n",
      "The concreteness score for 'division' is: 2.532\n"
     ]
    }
   ],
   "source": [
    "cord_context_conc = 0\n",
    "div_context_conc = 0\n",
    "\n",
    "'''\n",
    "Calculate the average concreteness of all contexts of the sense \"cord\" and \"division\".  Show that\n",
    "\"cord\" is higher.\n",
    "'''\n",
    "#Your code here\n",
    "def get_sense_avg_conc_score(sense):\n",
    "    '''\n",
    "    Calculate the average concreteness of all contexts of the sense in the training data\n",
    "    '''\n",
    "    context_list = train_dict[sense]\n",
    "    total = 0\n",
    "    for context in context_list:\n",
    "        total += get_conc_score(context)\n",
    "    return total/len(context_list)\n",
    "    \n",
    "cord_context_conc = get_sense_avg_conc_score('cord')\n",
    "div_context_conc = get_sense_avg_conc_score('division')\n",
    "\n",
    "#Your code here\n",
    "\n",
    "print(\"The concreteness score for 'cord' is: \" + str(round(cord_context_conc, 3)))\n",
    "print(\"The concreteness score for 'division' is: \" + str(round(div_context_conc, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gloss overlap features (Lesk)\n",
    "\n",
    "In this part you're going to apply the Lesk approach to WSD, looking for word overlap between the gloss of the sense and the context. However, you're not going to be able to use the version included in WordNet, for two reasons:\n",
    "\n",
    "1. We will be using a restricted set of senses, not all possible senses for *line* included in WordNet\n",
    "2. Rather than a single feature indicating which sense was chosen, we are going to calculate an overlap score for each possible sense\n",
    "\n",
    "To apply Lesk, you will first need to associate each sense in the Senseval dataset with a synset in WordNet. I've attached the most-likely synset to each sense in the Senseval dataset.  You are free to look at the definitions of the senses, and see how I arrived at those definitions.\n",
    "\n",
    "\n",
    "Write a function which takes a sentence context, and calculates the number of tokens that overlap between the context and the gloss of each sense in WordNet (HINT: use set intersection - we are only interested in *type* overlap). Your overlap calculation should exclude English stopwords (see COLX 521 Lecture 2). Your function should return a dictionary where the keys are senses and the values are overlap counts.\n",
    "\n",
    "Then, show that the average overlap of the \"product\" gloss from WordNet is higher with \"product\" contexts than \"division\" contexts.  Again, use the training data from Part 1.  At this point, you'll have a synset_dictionary (with the glosses from WordNet), and a list of contexts for each sense.  You can use these to calculate the average overlap of each sense in your context dictionary.\n",
    "\n",
    "For example, if your context dictionary has \"This line is busy\" and \"Hold the line\" for the sense 'phone', then you can calculate the overlap of \"This line is busy\" with the synset gloss of \"phone\", the same thing for the line \"Hold the line\", and then average them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cord', 'division', 'formation', 'phone', 'product', 'text'])\n"
     ]
    }
   ],
   "source": [
    "print(train_dict.keys())\n",
    "line_synsets = wn.synsets(\"line\")\n",
    "synset_lookup = {}\n",
    "synset_lookup['cord'] = wn.synset('line.n.18')\n",
    "synset_lookup['division'] = wn.synset('line.n.29')\n",
    "synset_lookup['formation'] = wn.synset('line.n.01')\n",
    "synset_lookup['phone'] = wn.synset('telephone_line.n.02')\n",
    "synset_lookup['product'] = wn.synset('line.n.22')\n",
    "synset_lookup['text'] = wn.synset('line.n.05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line.n.01\n",
      "a formation of people or things one beside another\n",
      "line.n.02\n",
      "a mark that is long relative to its width\n",
      "line.n.03\n",
      "a formation of people or things one behind another\n",
      "line.n.04\n",
      "a length (straight or curved) without breadth or thickness; the trace of a moving point\n",
      "line.n.05\n",
      "text consisting of a row of words written across a page or computer screen\n",
      "line.n.06\n",
      "a single frequency (or very narrow band) of radiation in a spectrum\n",
      "line.n.07\n",
      "a fortified position (especially one marking the most forward position of troops)\n",
      "argumentation.n.02\n",
      "a course of reasoning aimed at demonstrating a truth or falsehood; the methodical process of logical reasoning\n",
      "cable.n.02\n",
      "a conductor for transmitting electrical or optical signals or electric power\n",
      "course.n.02\n",
      "a connected series of events or actions or developments\n",
      "line.n.11\n",
      "a spatial location defined by a real or imaginary unidimensional extent\n",
      "wrinkle.n.01\n",
      "a slight depression in the smoothness of a surface\n",
      "pipeline.n.02\n",
      "a pipe used to transport liquids or gases\n",
      "line.n.14\n",
      "the road consisting of railroad track and roadbed\n",
      "telephone_line.n.02\n",
      "a telephone connection\n",
      "line.n.16\n",
      "acting in conformity\n",
      "lineage.n.01\n",
      "the descendants of one individual\n",
      "line.n.18\n",
      "something (as a cord or rope) that is long and thin and flexible\n",
      "occupation.n.01\n",
      "the principal activity in your life that you do to earn money\n",
      "line.n.20\n",
      "in games or sports; a mark indicating positions or bounds of the playing area\n",
      "channel.n.05\n",
      "(often plural) a means of communication or access\n",
      "line.n.22\n",
      "a particular kind of product or merchandise\n",
      "line.n.23\n",
      "a commercial organization serving as a common carrier\n",
      "agate_line.n.01\n",
      "space for one line of print (one column wide and 1/14 inch deep) used to measure advertising\n",
      "credit_line.n.01\n",
      "the maximum credit that a customer is allowed\n",
      "tune.n.01\n",
      "a succession of notes forming a distinctive sequence\n",
      "line.n.27\n",
      "persuasive but insincere talk that is usually intended to deceive or impress\n",
      "note.n.02\n",
      "a short personal letter\n",
      "line.n.29\n",
      "a conceptual separation or distinction\n",
      "production_line.n.01\n",
      "mechanical system in a factory whereby an article is conveyed through sites at which successive operations are performed on it\n",
      "line.v.01\n",
      "be in line with; form a line along\n",
      "line.v.02\n",
      "cover the interior of\n",
      "trace.v.02\n",
      "make a mark or lines on a surface\n",
      "line.v.04\n",
      "mark with lines\n",
      "line.v.05\n",
      "fill plentifully\n",
      "line.v.06\n",
      "reinforce with fabric\n",
      "SIZE:  36\n"
     ]
    }
   ],
   "source": [
    "for synset in line_synsets:\n",
    "    print(synset.name())\n",
    "    print(synset.definition())\n",
    "print(\"SIZE: \", len(line_synsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_overlap(context):\n",
    "    '''Calculate the number of tokens that overlap between the context \n",
    "    and the gloss of each sense in WordNet\n",
    "    '''\n",
    "    # Create a dictionary to store overlap\n",
    "    overlap_dict = defaultdict(int)\n",
    "    # Create a set to store all possible value in the word list\n",
    "    token_set = set()\n",
    "    for element in context:\n",
    "        # Skip ill-formatted word-pos pairs\n",
    "        if type(element) != tuple or len(element) != 2:\n",
    "            continue\n",
    "        word, _ = element\n",
    "        token_set.add(word.lower())\n",
    "    # Delete the English stop word from the set\n",
    "    token_set_clean = token_set - set(STOPWORDS)\n",
    "\n",
    "    # Then we need to get the gloss for each sense\n",
    "    for sense, synset in synset_lookup.items():\n",
    "        gloss_words_raw = synset.definition().split(\" \")\n",
    "        gloss_words = set()\n",
    "        for word in gloss_words_raw:\n",
    "            # Get rid of bracket\n",
    "            word = word.replace(\"(\", \"\")\n",
    "            word = word.replace(\")\", \"\")\n",
    "            gloss_words.add(word.lower())\n",
    "        overlap_dict[sense] = len(token_set_clean.intersection(set(gloss_words)))\n",
    "    return overlap_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = \"I was holding a flexible line\"\n",
    "test_context = pos_tag(word_tokenize(test_sent))\n",
    "assert count_overlap(test_context)['cord'] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average overlap of the 'product' gloss with the product contexts is: 0.059\n",
      "The average overlap of the 'product' gloss with the division contexts is: 0.017\n",
      "{'product': 0.059494298463063956, 'division': 0.017241379310344827}\n"
     ]
    }
   ],
   "source": [
    "# 'Product' gloss overlap with product contexts\n",
    "avg_overlap_dict = {}\n",
    "avg_overlap_dict['product'] = 0\n",
    "avg_overlap_dict['division'] = 0\n",
    "\n",
    "def get_sense_avg_overlap(sense):\n",
    "    '''\n",
    "    Calculate the average overlap of all contexts of the sense in the training data\n",
    "    '''\n",
    "    context_list = train_dict[sense]\n",
    "    total = 0\n",
    "    for context in context_list:\n",
    "        total += count_overlap(context)[sense]\n",
    "    return total/len(context_list)\n",
    "\n",
    "#Your code here\n",
    "avg_overlap_dict['product'] = get_sense_avg_overlap('product')\n",
    "#Your code here\n",
    "avg_overlap_dict['division'] = get_sense_avg_overlap('division')\n",
    "\n",
    "print(f\"The average overlap of the 'product' gloss with the product contexts is: {avg_overlap_dict['product']:0.3f}\")\n",
    "print(f\"The average overlap of the 'product' gloss with the division contexts is: {avg_overlap_dict['division']:0.3f}\")\n",
    "print(avg_overlap_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 : WordNet distance features\n",
    "rubric={accuracy:5,efficiency:1,quality:1}\n",
    "\n",
    "This feature involves calculating the WordNet (Wupalmer) distances from the synsets of relevant senses of *line* to the synsets of mostly non-ambiguous context words. For this, you will need the Senseval -> WordNet sense mapping from 2.2.  If you can't remember how to get the Wupalmer (ie, wup) value, check the lecture slides.\n",
    "\n",
    "The biggest challenge in this problem is identifying \"mostly\" non-ambiguous words. We could exclude any word type that has any polysemy (i.e. associated with more than one synset), but that seems too extreme (almost all words have some rare instances of strange sense uses). Instead, we are going to consider a word mostly non-ambiguous if it appears as one particular sense 75% of the time, based on the corpus counts provided in WordNet. You should write a general function, `get_dominant_sense`, which takes a word and a POS (a single letter, same as the input to the WordNet lemmatizer), and returns the dominant (75% of instances) synset if it exists, or `None` if it doesn't. The POS will be useful because, in order to do this properly, you will have to correctly lemmatize the word, so as to match it with the lemmas of each of its synset, so you can get the right count.\n",
    "\n",
    "So this function should take a word and pos as input, and then: <br/>\n",
    "1. Lemmatize the word <br/>\n",
    "2. Get all the senses from the WordNet synsets for the word <br/>\n",
    "3. Keep track of the counts for each sense that match the lemma <br/>\n",
    "4. If the highest count is greater than 0.75 * total count, then return that synset.  Otherwise, return None. <br/>\n",
    "\n",
    "Once you have this function, you should create another function which will, for a particular instance,\n",
    "\n",
    "1. Use `get_dominant_sense` to get a list of synsets appearing in the context (one for each mostly non-ambiguous word). You will need to do this again in 2.4, so a separate function might be a good idea!  The function will take a context as input, and return a list of synsets.\n",
    "2. For each sense of *line* in Senseval (ie, the senses in synset_lookup), calculate the average distance between that sense and all the synsets in the context. You should use the built-in function for calculating Wu-Palmer distance between a synset pair, don't implement your own.  \n",
    "3. Return a dictionary mapping the (Senseval) sense to the average distance to the context synset.  That is, return a dictionary where the keys are the six senses in synset_lookup, and the values are the average distance from the context to that sense.\n",
    "\n",
    "Then use the output of this function to show that the synsets associated with contexts around \"phone\" sense of line are on average closer to the \"phone\" synset than synsets from \"division\" contexts are.\n",
    "\n",
    "That is, for each context in your training dictionary with the sense \"phone\", calculate the average distance between the context and the \"phone\" sense in synset_lookup.  Then, do the same for each context in your training dictionary with the sense \"division\".  Show that the \"phone\" sense is closer for \"phone\" contexts than \"division\" contexts (\"closer\" means the number will be smaller - this is a distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_sense_ratio = 0.75\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_dominant_sense(word, pos=\"n\"):\n",
    "    \"\"\"return the dominant (75% of instances) synset of the word if it exists, or None if it doesn't\n",
    "    word -- an English word\n",
    "    pos -- a single letter that represents part of speech of the input word, noun by default\"\"\"\n",
    "\n",
    "    # Your code here\n",
    "    # The return value\n",
    "    return_dominant_sense = None\n",
    "    if pos =='n' or pos =='v' or pos == 'a' or pos == 'r':\n",
    "        word = lemmatizer.lemmatize(word.lower(), pos)\n",
    "    else:\n",
    "        word = lemmatizer.lemmatize(word.lower())\n",
    "    goal_synsets = wn.synsets(word)\n",
    "\n",
    "    # Create a dictionary to store counts for each sense that match the lemma\n",
    "    synset_dict = {}\n",
    "    for synset in goal_synsets:\n",
    "        for lemma in synset.lemmas():\n",
    "            if (lemma.name() == word):\n",
    "                synset_dict[synset.name()] = lemma.count()\n",
    "\n",
    "    total_count = sum(synset_dict.values())\n",
    "    if len(synset_dict) != 0:\n",
    "        max_item = max(synset_dict.items(), key=lambda x: x[1])\n",
    "        highest_count = max_item[1]\n",
    "    if (total_count != 0) and (highest_count / total_count > dominant_sense_ratio):\n",
    "        return_dominant_sense = wn.synset(max_item[0])\n",
    "\n",
    "    return return_dominant_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_dominant_sense('word', 'n').name() == 'word.n.01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ideally.r.01'),\n",
       " Synset('state.v.01'),\n",
       " Synset('buy.v.01'),\n",
       " Synset('strong.a.01'),\n",
       " Synset('cash.n.01'),\n",
       " Synset('compatible.a.01')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_into_wordnet_pos_code(tag):\n",
    "    \"\"\"return the wordnet pos code, by mapping from the NTLK POS tags\"\"\"\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith(\"VB\"):\n",
    "        return wn.VERB\n",
    "    elif tag.startswith(\"JJ\"):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith(\"RB\"):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_dominant_sense_context(context):\n",
    "    \"\"\"return a list of dominant synsets appearing in the context,\n",
    "    one for each mostly non-ambiguous word\"\"\"\n",
    "\n",
    "    # Your code here\n",
    "    valid_context = []\n",
    "    dominant_synsets = []\n",
    "    for element in context:\n",
    "        # Skip ill-formatted word-pos pairs\n",
    "        if type(element) != tuple or len(element) != 2:\n",
    "            continue\n",
    "        word, tag = element\n",
    "        if word.lower() in set(STOPWORDS):\n",
    "            continue\n",
    "        if not tag.isalpha():\n",
    "            continue\n",
    "        else:\n",
    "            valid_context.append(element)\n",
    "\n",
    "    for valid_word, valid_tag in valid_context:\n",
    "        wordnet_pos = convert_into_wordnet_pos_code(valid_tag)\n",
    "        dominant_sense_synset = get_dominant_sense(valid_word, wordnet_pos)\n",
    "        # print(valid_word, valid_tag, wordnet_pos, dominant_sense_synset)\n",
    "        if dominant_sense_synset is not None:\n",
    "            dominant_synsets.append(dominant_sense_synset)\n",
    "\n",
    "    return dominant_synsets\n",
    "\n",
    "\n",
    "get_dominant_sense_context(train_dict[\"product\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cord': 0.7984126984126984,\n",
       " 'division': 0.8261689291101054,\n",
       " 'formation': 0.7746031746031746,\n",
       " 'phone': 0.8327020202020202,\n",
       " 'product': 0.8327020202020202,\n",
       " 'text': 0.8118686868686869}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_average_distance(context):\n",
    "    \"\"\"calculate average distance between senses of the word \"line\" and a context\"\"\"\n",
    "\n",
    "    # Your code here\n",
    "    # Create a dictionary to store differences between senses and a context\n",
    "    distance_diff_dict = defaultdict(int)\n",
    "    dominant_synsets = get_dominant_sense_context(context)\n",
    "    if len(dominant_synsets) == 0:\n",
    "        return {}\n",
    "    for key, value in synset_lookup.items():\n",
    "        for synset in dominant_synsets:\n",
    "            distance_diff_dict[key] += 1 - value.wup_similarity(synset)\n",
    "\n",
    "    total_iters = len(dominant_synsets)\n",
    "    return_avg_distance = {k: v / total_iters for k, v in distance_diff_dict.items()}\n",
    "    return return_avg_distance\n",
    "\n",
    "\n",
    "get_average_distance(train_dict[\"product\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(get_average_distance([('hooks', 'NNP'), ('fly', 'VB'), ('in', 'IN'), ('every', 'DT'), ('direction', 'NN'), ('.', '.'), ('lines', 'NNS'), ('become', 'VBP'), ('tangled', 'JJ'), ('.', '.')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance between the synsets associated with contexts around 'phone' sense of line and 'phone' synset:  0.761\n"
     ]
    }
   ],
   "source": [
    "avg_phone_distance = 0\n",
    "count = 0\n",
    "\n",
    "# Your code here\n",
    "phone_contexts = train_dict[\"phone\"]\n",
    "for phone_context in phone_contexts:\n",
    "    temp_dict = get_average_distance(phone_context)\n",
    "    if len(temp_dict) != 0:\n",
    "        avg_phone_distance += temp_dict[\"phone\"]\n",
    "        count += 1\n",
    "\n",
    "print(\n",
    "    \"The distance between the synsets associated with contexts around 'phone' sense of line and 'phone' synset: \",\n",
    "    round(avg_phone_distance / count, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance between the synsets associated with contexts around 'division' sense of line and 'phone' synset:  0.787\n"
     ]
    }
   ],
   "source": [
    "avg_division_distance = 0\n",
    "count = 0\n",
    "\n",
    "# Your code here\n",
    "phone_contexts = train_dict[\"division\"]\n",
    "for phone_context in phone_contexts:\n",
    "    temp_dict = get_average_distance(phone_context)\n",
    "    if len(temp_dict) != 0:\n",
    "        avg_division_distance += temp_dict[\"phone\"]\n",
    "        count += 1\n",
    "\n",
    "print(\n",
    "    \"The distance between the synsets associated with contexts around 'division' sense of line and 'phone' synset: \",\n",
    "    round(avg_division_distance / count, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordNet Hypernyms\n",
    "\n",
    "Now, we will consider the count of WordNet synsets in the context directly as features. However, limiting ourselves to the synsets corresponding directly to words might result in sparsity, and provide little more information than raw words would. Instead, we are going to also include all the hypernyms of words appearing in the context as potential features for doing WSD.\n",
    "\n",
    "First, we write a recursive function `get_all_hypernyms` which collects the names (e.g. `synset.name()`) of a provided WordNet synset and all of its hypernyms.  The base case can just be when an item no longer has any hypernyms.\n",
    "\n",
    "Then, applying this function to the synsets found in the context (step 1 of the distance function in 2.3), write a function that counts all the hypernyms of all the (again mostly non-ambiguous) synsets in the context, normalizing by the total count to get a proportion for each synset.\n",
    "\n",
    "The function will take a context as input.  It will calculate the dominant synsets from this context.  Then, for each of these synsets, it will get all of their hypernyms, and keep track of their counts.\n",
    "The returned dictionary will have the hypernym names as keys, and the percentage of all the hypernyms found using this method.  For example, if you count all the hypernyms, and have 20, and 5 of them are \"animal\", then \"animal\" will have a value of \"0.25\"\n",
    "\n",
    "Then we show that the average proportion of the 'object.n.01' synset is higher in contexts involving the \"cord\" sense of *line* than the \"division\" sense. (This should be true for the same reason as in 2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hypernyms(synset, names=[]):\n",
    "    '''return a list of the names of a synset and all its hypernyms'''\n",
    "    #Your code here\n",
    "    names.append(str(synset.name()))\n",
    "    synset_hypernyms= synset.hypernyms()\n",
    "    #Base Case\n",
    "    if(len(synset_hypernyms)==0):\n",
    "        return names\n",
    "    else:\n",
    "        for hypernym in synset_hypernyms:\n",
    "            get_all_hypernyms(hypernym, names)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_synset = wn.synset(\"cat.n.01\")\n",
    "assert get_all_hypernyms(cat_synset, []) == [\n",
    "    \"cat.n.01\",\n",
    "    \"feline.n.01\",\n",
    "    \"carnivore.n.01\",\n",
    "    \"placental.n.01\",\n",
    "    \"mammal.n.01\",\n",
    "    \"vertebrate.n.01\",\n",
    "    \"chordate.n.01\",\n",
    "    \"animal.n.01\",\n",
    "    \"organism.n.01\",\n",
    "    \"living_thing.n.01\",\n",
    "    \"whole.n.02\",\n",
    "    \"object.n.01\",\n",
    "    \"physical_entity.n.01\",\n",
    "    \"entity.n.01\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_of_all_hypernyms(context):\n",
    "    '''get all the hypernyms of all the dominent synsets in a given context,\n",
    "       return normalized counts'''\n",
    "    hypernyms = {}\n",
    "    #Your code here\n",
    "    total_count =0\n",
    "    for synset in get_dominant_sense_context(context):\n",
    "        for hypernym in get_all_hypernyms(synset):\n",
    "            hypernyms[hypernym]= hypernyms.get(hypernym, 0) + 1\n",
    "            total_count += 1\n",
    "    for hypernym, count in hypernyms.items():\n",
    "        hypernyms[hypernym]= count/total_count\n",
    "    return hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hear.v.01': 0.05952380952380952,\n",
       " 'perceive.v.01': 0.05952380952380952,\n",
       " 'rebuff.n.01': 0.047619047619047616,\n",
       " 'discourtesy.n.03': 0.047619047619047616,\n",
       " 'behavior.n.01': 0.047619047619047616,\n",
       " 'activity.n.01': 0.047619047619047616,\n",
       " 'act.n.02': 0.047619047619047616,\n",
       " 'event.n.01': 0.047619047619047616,\n",
       " 'psychological_feature.n.01': 0.047619047619047616,\n",
       " 'abstraction.n.06': 0.07142857142857142,\n",
       " 'entity.n.01': 0.11904761904761904,\n",
       " 'beach.n.01': 0.03571428571428571,\n",
       " 'geological_formation.n.01': 0.03571428571428571,\n",
       " 'object.n.01': 0.047619047619047616,\n",
       " 'physical_entity.n.01': 0.047619047619047616,\n",
       " 'inch.n.01': 0.023809523809523808,\n",
       " 'linear_unit.n.01': 0.023809523809523808,\n",
       " 'unit_of_measurement.n.01': 0.023809523809523808,\n",
       " 'definite_quantity.n.01': 0.023809523809523808,\n",
       " 'measure.n.02': 0.023809523809523808,\n",
       " 'bucket.n.01': 0.011904761904761904,\n",
       " 'vessel.n.03': 0.011904761904761904,\n",
       " 'container.n.01': 0.011904761904761904,\n",
       " 'instrumentality.n.03': 0.011904761904761904,\n",
       " 'artifact.n.01': 0.011904761904761904,\n",
       " 'whole.n.02': 0.011904761904761904}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_of_all_hypernyms(train_dict['cord'][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(get_all_of_all_hypernyms(train_dict['cord'][16]).values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On average, are the synsets associated with contexts around \"phone\" sense of `line` closer to the \"phone\" synset than synsets from \"division\" contexts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg proportion of the 'object.n.01' synset in 'cord' contexts:  0.055\n"
     ]
    }
   ],
   "source": [
    "# average proportion of the \"object.n.01\" synset in \"cord\" contexts\n",
    "\n",
    "total_prop = 0\n",
    "count = 0\n",
    "for context in train_dict['cord']:\n",
    "    hypernyms = get_all_of_all_hypernyms(context)\n",
    "    if \"object.n.01\" in get_all_of_all_hypernyms(context).keys():\n",
    "        total_prop += hypernyms[\"object.n.01\"]\n",
    "        count += 1\n",
    "\n",
    "print(\"avg proportion of the 'object.n.01' synset in 'cord' contexts: \", round(total_prop / count, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg proportion of the 'object.n.01' synset in 'division' contexts:  0.045\n"
     ]
    }
   ],
   "source": [
    "# average proportion of the \"object.n.01\" synset in \"division\" contexts\n",
    "\n",
    "total_prop = 0\n",
    "count = 0\n",
    "for context in train_dict['division']:\n",
    "    hypernyms = get_all_of_all_hypernyms(context)\n",
    "    if \"object.n.01\" in hypernyms.keys():\n",
    "        total_prop += hypernyms[\"object.n.01\"]\n",
    "        count += 1\n",
    "\n",
    "print(\"avg proportion of the 'object.n.01' synset in 'division' contexts: \", round(total_prop / count, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a classifier\n",
    "\n",
    "Now that we have a collection of features which show some promise for the task, we build a classifier for WSD of *line* which uses all the features above. We combine all the individual outputs of the functions into a single feature dictionary for each context sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dict(context):\n",
    "    '''Extract a feature dictionary for an input text'''\n",
    "    feature_dict = {}\n",
    "    # Your code here\n",
    "    # Calculate the correctness\n",
    "    feature_dict['concreteness'] = get_conc_score(context)\n",
    "    # Calculate the overlap and avg distance\n",
    "    overlaps = count_overlap(context)\n",
    "    avg_distances = get_average_distance(context)\n",
    "    # Add the dict to the feature dict\n",
    "    for sense in list(synset_lookup.keys()):\n",
    "        feature_dict['_'.join(['overlap', sense])] = overlaps[sense]\n",
    "        if len(avg_distances) != 0:\n",
    "            feature_dict['_'.join(['avg_dist', sense])] = avg_distances[sense]\n",
    "    # Add Hypernyms\n",
    "    hypernyms = get_all_of_all_hypernyms(context)\n",
    "    for hypernym, prop in hypernyms.items():\n",
    "        feature_dict['_'.join(['avg_prop_hypernym', hypernym])] = prop\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_dicts = []\n",
    "train_classification = []\n",
    "test_feat_dicts = []\n",
    "test_classification = []\n",
    "\n",
    "for key in test_dict.keys():\n",
    "    for context in range(len(test_dict[key])):\n",
    "        test_feat_dicts.append(get_feature_dict(test_dict[key][context]))\n",
    "        test_classification.append(key)\n",
    "\n",
    "try:\n",
    "    for key in train_dict.keys():\n",
    "        for context in range(len(train_dict[key])):\n",
    "            train_feat_dicts.append(get_feature_dict(train_dict[key][context]))\n",
    "            train_classification.append(key)\n",
    "except IndexError:\n",
    "    print(IndexError)\n",
    "    print(train_dict[key][context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(train_dict, test_dict):\n",
    "    '''vectorize given lists of feature dictionaries, return X_train and X_test'''\n",
    "    vectorizer = DictVectorizer(sparse=False, dtype=float)\n",
    "    X_train = vectorizer.fit_transform(train_dict)\n",
    "    X_test = vectorizer.transform(test_dict)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = vectorize(train_feat_dicts, test_feat_dicts)\n",
    "y_train, y_test = train_classification, test_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feat_dicts\n",
    "# Comment out this line because the output is too large to push to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score:  0.16833333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Final score: \", tree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment: Taking out `average_distance` features (WordNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_average_distance_feature(feat_dicts):\n",
    "    '''remove average distance feature from data set'''\n",
    "    new_feat_dicts = []\n",
    "\n",
    "    #Your code here\n",
    "    for feat_dict in feat_dicts:\n",
    "        new_feat_dict = {}\n",
    "        for feature, value in feat_dict.items():\n",
    "            if feature[:8] != 'avg_dist':\n",
    "                new_feat_dict[feature] = value\n",
    "        new_feat_dicts.append(new_feat_dict)\n",
    "    \n",
    "    return new_feat_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_feat_dicts, new_test_feat_dicts = remove_average_distance_feature(train_feat_dicts), remove_average_distance_feature(test_feat_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_test_feat_dicts\n",
    "# Comment out this line because the output is too large to push to GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_train, X_new_test = vectorize(new_train_feat_dicts, new_test_feat_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "tree.fit(X_new_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with new training set:  0.16833333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Score with new training set: \", tree.score(X_new_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to take out the average distance feature from the data sets because it was the feature that showed the least significant distinction between different senses of contexts. As a result, we got a slightly higher score compared to the final score with all features. Given that feature extraction takes a significant amount of time, it would be better to take out the feature from the data sets and try to implement other features."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8412fb3a3e72ead114876fbb122a05c15918d5c78bd880026c0525133725798a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
